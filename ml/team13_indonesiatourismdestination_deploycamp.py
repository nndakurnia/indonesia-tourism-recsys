# -*- coding: utf-8 -*-
"""Team13-IndonesiaTourismDestination-DeployCamp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QJHgBLu4KV-eOyLXyyjuHVSRiectSojt

# DATA LOADING DAN INITIAL EXPLORATION
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

plt.style.use('default')
sns.set_palette("husl")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Upload files ke Colab
from google.colab import files

df_packages = pd.read_csv('/content/drive/MyDrive/Datasets_DeployCamp/package_tourism.csv')
df_ratings = pd.read_csv('/content/drive/MyDrive/Datasets_DeployCamp/tourism_rating.csv')
df_tourism = pd.read_csv('/content/drive/MyDrive/Datasets_DeployCamp/tourism_with_id.csv')
df_users = pd.read_csv('/content/drive/MyDrive/Datasets_DeployCamp/user.csv')

# Melihat isi dataset
print("="*50 + "\ndf_packages:\n" + "="*50)
display(df_packages.head())

print("\n" + "="*50 + "\ndf_ratings:\n" + "="*50)
display(df_ratings.head())

print("\n" + "="*50 + "\ndf_tourism:\n" + "="*50)
display(df_tourism.head())

print("\n" + "="*50 + "\ndf_users:\n" + "="*50)
display(df_users.head())

print(f"Package data shape: {df_packages.shape}")
print(f"Rating data shape: {df_ratings.shape}")
print(f"Tourism data shape: {df_tourism.shape}")
print(f"User data shape: {df_users.shape}")

"""# EXPLORATORY DATA ANALYSIS (EDA)"""

# Check missing values
print(f"Package data: {df_packages.isnull().sum().sum()} missing values")
print(f"Rating data: {df_ratings.isnull().sum().sum()} missing values")
print(f"Tourism data: {df_tourism.isnull().sum().sum()} missing values")
print(f"User data: {df_users.isnull().sum().sum()} missing values")

# Analisis statistik dasar untuk data ratings
print("RATING STATISTICS:")
print("-" * 25)
if 'Place_Ratings' in df_ratings.columns:
    rating_stats = df_ratings['Place_Ratings'].describe()
    print(f"   Average rating: {rating_stats['mean']:.2f}")
    print(f"   Median rating: {rating_stats['50%']:.2f}")
    print(f"   Min rating: {rating_stats['min']}")
    print(f"   Max rating: {rating_stats['max']}")
    print(f"   Standard deviation: {rating_stats['std']:.2f}")

    # Distribusi rating
    rating_dist = df_ratings['Place_Ratings'].value_counts().sort_index()
    print(f"   Rating distribution: {rating_dist}")

print()

# Analisis data user
print("USER STATISTICS:")
print("-" * 20)
if 'Age' in df_users.columns:
    age_stats = df_users['Age'].describe()
    print(f"   Total unique users: {df_users['User_Id'].nunique():,}")
    print(f"   Average age: {age_stats['mean']:.1f} years")
    print(f"   Age range: {age_stats['min']:.0f} - {age_stats['max']:.0f} years")
    print(f"   Median age: {age_stats['50%']:.1f} years")

print()

# Analisis tempat wisata
print("TOURISM PLACES STATISTICS:")
print("-" * 30)
print(f"Total unique places: {df_tourism['Place_Id'].nunique():,}")
if 'Category' in df_tourism.columns:
    print(f"Categories: {df_tourism['Category'].nunique()}")
    cat_dist = df_tourism['Category'].value_counts()
    print(f"\nTop categories: {cat_dist.head(3)}")
if 'City' in df_tourism.columns:
    print(f"\nCities: {df_tourism['City'].nunique()}")
    city_dist = df_tourism['City'].value_counts()
    print(f"Top cities: {city_dist.head(3)}")

print()

"""# DATA VISUALIZATION  """

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Indonesia Tourism Data Analysis', fontsize=16, fontweight='bold')

# Rating distribution
axes[0, 0].hist(df_ratings['Place_Ratings'], bins=5, color='skyblue', edgecolor='black')
axes[0, 0].set_title('Distribusi Rating Tempat Wisata', fontweight='bold')
axes[0, 0].set_xlabel('Rating')
axes[0, 0].set_ylabel('Jumlah Rating')
axes[0, 0].grid(True, alpha=0.3)

# User age distribution
axes[0, 1].hist(df_users['Age'], bins=15, color='lightgreen', edgecolor='black')
axes[0, 1].set_title('User Age Distribution')
axes[0, 1].set_xlabel('Umur (tahun)')
axes[0, 1].set_ylabel('Jumlah User')
axes[0, 1].grid(True, alpha=0.3)

# Top 10 cities
top_cities = df_tourism['City'].value_counts().head(10)
axes[1, 0].bar(range(len(top_cities)), top_cities.values, color='coral')
axes[1, 0].set_title('Top 10 Kota Wisata', fontweight='bold')
axes[1, 0].set_xlabel('Kota')
axes[1, 0].set_ylabel('Jumlah Tempat Wisata')
axes[1, 0].set_xticks(range(len(top_cities)))
axes[1, 0].set_xticklabels(top_cities.index, rotation=360)
axes[1, 0].grid(True, alpha=0.3)

# Categories distribution
categories = df_tourism['Category'].value_counts()
axes[1, 1].pie(categories.values, labels=categories.index, autopct='%1.1f%%')
axes[1, 1].set_title('Distribusi Kategori Tempat Wisata')

plt.tight_layout()
plt.show()

"""# DATA CLEANING DAN PREPARATION"""

df_tourism_clean = df_tourism.copy()

# Perbaiki nama kolom yang aneh
df_tourism_clean = df_tourism_clean.rename(columns={'Unnamed: 12': 'User_Id'})

# Hapus kolom yang tidak berguna
df_tourism_clean = df_tourism_clean.drop(columns=['Unnamed: 11'])

df_tourism_clean.head()

# Handle missing values di dataset tourism
df_tourism['Rating'] = df_tourism['Rating'].fillna(df_tourism['Rating'].mean())
df_tourism['Time_Minutes'] = df_tourism['Time_Minutes'].fillna(df_tourism['Time_Minutes'].median())

"""# FEATURE ENGINEERING

## PRICE PROCESSING
"""

# Convert Price ke format numeric yang bersih
if 'Price' in df_tourism_clean.columns:
    # Buat kolom Price_Numeric yang bersih
    df_tourism_clean['Price_Numeric'] = 0.0

    # Proses setiap baris price
    price_converted = 0
    price_errors = 0

    for idx, price_val in df_tourism_clean['Price'].items():
        try:
            if pd.notna(price_val):
                # Convert ke string dan bersihkan
                price_str = str(price_val).replace('Rp', '').replace('.', '').replace(',', '').strip()

                if price_str and price_str != 'nan' and price_str.replace('.', '').isdigit():
                    df_tourism_clean.loc[idx, 'Price_Numeric'] = float(price_str)
                    price_converted += 1
                else:
                    df_tourism_clean.loc[idx, 'Price_Numeric'] = 0.0
            else:
                df_tourism_clean.loc[idx, 'Price_Numeric'] = 0.0
        except:
            df_tourism_clean.loc[idx, 'Price_Numeric'] = 0.0
            price_errors += 1

    print(f"{price_converted:,} harga berhasil dikonversi")
    print(f"{price_errors} error dalam konversi harga")

    # Buat kategori harga berdasarkan distribusi
    price_values = df_tourism_clean['Price_Numeric'][df_tourism_clean['Price_Numeric'] > 0]

    if len(price_values) > 0:
        price_q33 = price_values.quantile(0.33)
        price_q66 = price_values.quantile(0.66)

        # Definisikan kondisi untuk kategori harga
        conditions = [
            df_tourism_clean['Price_Numeric'] == 0,
            df_tourism_clean['Price_Numeric'] <= price_q33,
            df_tourism_clean['Price_Numeric'] <= price_q66,
            df_tourism_clean['Price_Numeric'] > price_q66
        ]
        choices = ['Free', 'Budget', 'Mid_Range', 'Premium']
        df_tourism_clean['price_category'] = np.select(conditions, choices, default='Unknown')

        print(f"\nKategori harga dibuat:")
        print(f"   - Free: Rp 0")
        print(f"   - Budget: Rp 1 - {price_q33:,.0f}")
        print(f"   - Mid_Range: Rp {price_q33:,.0f} - {price_q66:,.0f}")
        print(f"   - Premium: > Rp {price_q66:,.0f}")

        # Tampilkan distribusi kategori harga
        price_cat_dist = df_tourism_clean['price_category'].value_counts()
        print(f"\nDistribusi: {price_cat_dist}")
    else:
        df_tourism_clean['price_category'] = 'Unknown'
        print("Tidak ada data harga valid untuk kategorisasi")

print()

"""## GEOGRAPHIC FEATURES"""

# Key learning: Invest 40% of project time in data quality
quality_pipeline_lessons = {
    'missing_values': "Statistical imputation outperforms simple fill methods",
    'outlier_detection': "Domain knowledge essential for meaningful outlier rules",
    'categorical_encoding': "Label encoding sufficient for tree-based models",
    'data_validation': "Automated checks prevent downstream pipeline failures"
}

# Mapping kota ke provinsi untuk analisis regional lebih baik
city_to_province = {
    'Jakarta': 'DKI Jakarta',
    'Bandung': 'Jawa Barat',
    'Surabaya': 'Jawa Timur',
    'Semarang': 'Jawa Tengah',
    'Medan': 'Sumatera Utara',
    'Makassar': 'Sulawesi Selatan',
    'Denpasar': 'Bali',
    'Palembang': 'Sumatera Selatan',
    'Yogyakarta': 'DI Yogyakarta',
    'Balikpapan': 'Kalimantan Timur',
    'Manado': 'Sulawesi Utara',
    'Pontianak': 'Kalimantan Barat',
    'Malang': 'Jawa Timur',
    'Solo': 'Jawa Tengah',
    'Bogor': 'Jawa Barat',
    'Batam': 'Kepulauan Riau',
    'Pekanbaru': 'Riau',
    'Padang': 'Sumatera Barat',
    'Samarinda': 'Kalimantan Timur',
    'Banjarmasin': 'Kalimantan Selatan'
}

# Tambahkan kolom province
if 'City' in df_tourism_clean.columns:
    df_tourism_clean['province'] = df_tourism_clean['City'].map(city_to_province)

    # Isi yang tidak ada mapping dengan 'Other'
    df_tourism_clean['province'] = df_tourism_clean['province'].fillna('Other')

    province_dist = df_tourism_clean['province'].value_counts()
    print(f"Distribusi provinsi: {province_dist.head()}")

print()

"""## USER FEATURE ENGINEERING  """

# Membuat age groups
bins = [0, 18, 25, 35, 45, 55, 100]
labels = ['Teen', 'Young_Adult', 'Adult', 'Middle_Age', 'Senior', 'Elder']

df_users_enhanced = df_users.copy()
if 'Age' in df_users_enhanced.columns:
    df_users_enhanced['age_group'] = pd.cut(df_users_enhanced['Age'],
                                          bins=bins, labels=labels,
                                          include_lowest=True)

    age_group_dist = df_users_enhanced['age_group'].value_counts()
    print(f"Age groups dibuat: {age_group_dist}")

# Hitung statistik rating per user untuk profiling
user_rating_stats = df_ratings.groupby('User_Id').agg({
    'Place_Ratings': ['count', 'mean', 'std', 'min', 'max'],
    'Place_Id': 'nunique'
}).round(4)

# Flatten column names
user_rating_stats.columns = [
    'user_total_ratings', 'user_avg_rating', 'user_rating_std',
    'user_min_rating', 'user_max_rating', 'user_unique_places'
]

# Handle NaN di standard deviation
user_rating_stats['user_rating_std'] = user_rating_stats['user_rating_std'].fillna(0)

# Buat derived features yang berguna untuk recommendation
user_rating_stats['user_rating_range'] = (user_rating_stats['user_max_rating'] -
                                         user_rating_stats['user_min_rating'])

user_rating_stats['user_diversity_ratio'] = (user_rating_stats['user_unique_places'] /
                                            user_rating_stats['user_total_ratings'])

# Buat behavioral flags
user_rating_stats['user_is_generous'] = (user_rating_stats['user_avg_rating'] > 3.5).astype(int)
user_rating_stats['user_is_active'] = (user_rating_stats['user_total_ratings'] >
                                      user_rating_stats['user_total_ratings'].median()).astype(int)
user_rating_stats['user_is_consistent'] = (user_rating_stats['user_rating_std'] < 1.0).astype(int)
user_rating_stats['user_is_explorer'] = (user_rating_stats['user_diversity_ratio'] > 0.8).astype(int)

print(f"User stats shape: {user_rating_stats.shape}")

# Gabungkan dengan data user
user_features_complete = df_users_enhanced.merge(user_rating_stats.reset_index(),
                                                on='User_Id', how='left')

# Fill NaN untuk user yang belum pernah rating
rating_columns = ['user_total_ratings', 'user_avg_rating', 'user_rating_std',
                 'user_min_rating', 'user_max_rating', 'user_unique_places',
                 'user_rating_range', 'user_diversity_ratio']

for col in rating_columns:
    user_features_complete[col] = user_features_complete[col].fillna(0)

behavioral_columns = ['user_is_generous', 'user_is_active',
                     'user_is_consistent', 'user_is_explorer']
for col in behavioral_columns:
    user_features_complete[col] = user_features_complete[col].fillna(0)

print(f"Complete user features shape: {user_features_complete.shape}")
print()

"""## PLACE FEATURE ENGINEERING"""

# Hitung statistik rating per tempat
place_rating_stats = df_ratings.groupby('Place_Id').agg({
    'Place_Ratings': ['count', 'mean', 'std', 'min', 'max'],
    'User_Id': 'nunique'
}).round(4)

# Flatten column names
place_rating_stats.columns = [
    'place_total_ratings', 'place_avg_rating', 'place_rating_std',
    'place_min_rating', 'place_max_rating', 'place_unique_users'
]

# Handle NaN
place_rating_stats['place_rating_std'] = place_rating_stats['place_rating_std'].fillna(0)

# Derived features untuk places
place_rating_stats['place_rating_range'] = (place_rating_stats['place_max_rating'] -
                                           place_rating_stats['place_min_rating'])

place_rating_stats['place_user_diversity'] = (place_rating_stats['place_unique_users'] /
                                             place_rating_stats['place_total_ratings'])

# Behavioral flags untuk places
place_rating_stats['place_is_popular'] = (place_rating_stats['place_total_ratings'] >
                                         place_rating_stats['place_total_ratings'].median()).astype(int)

place_rating_stats['place_is_highly_rated'] = (place_rating_stats['place_avg_rating'] > 4.0).astype(int)

place_rating_stats['place_is_consistent'] = (place_rating_stats['place_rating_std'] < 1.0).astype(int)

place_rating_stats['place_is_niche'] = (place_rating_stats['place_unique_users'] < 10).astype(int)

# Popularity score yang menggabungkan rating dan jumlah rating
place_rating_stats['popularity_score'] = (place_rating_stats['place_avg_rating'] *
                                         np.log1p(place_rating_stats['place_total_ratings'])).round(4)

print(f"Place stats shape: {place_rating_stats.shape}")

# Gabungkan dengan data tourism
place_features_complete = df_tourism_clean.merge(place_rating_stats.reset_index(),
                                                on='Place_Id', how='left')

# Fill NaN untuk places tanpa rating
place_rating_columns = ['place_total_ratings', 'place_avg_rating', 'place_rating_std',
                       'place_min_rating', 'place_max_rating', 'place_unique_users',
                       'place_rating_range', 'place_user_diversity', 'popularity_score']

for col in place_rating_columns:
    place_features_complete[col] = place_features_complete[col].fillna(0)

place_behavioral_columns = ['place_is_popular', 'place_is_highly_rated',
                           'place_is_consistent', 'place_is_niche']
for col in place_behavioral_columns:
    place_features_complete[col] = place_features_complete[col].fillna(0)

print(f"Complete place features shape: {place_features_complete.shape}")
print()

"""# INTERACTION DATASET CREATION"""

# Mulai dengan data ratings sebagai base
ml_dataset = df_ratings.copy()
print(f"Base interactions: {ml_dataset.shape[0]:,}")

# Merge dengan user features
user_columns_to_merge = ['User_Id', 'Age', 'Location', 'age_group',
                        'user_avg_rating', 'user_rating_std', 'user_total_ratings',
                        'user_is_generous', 'user_is_active', 'user_is_consistent',
                        'user_is_explorer']

available_user_cols = [col for col in user_columns_to_merge if col in user_features_complete.columns]
ml_dataset = ml_dataset.merge(user_features_complete[available_user_cols],
                             on='User_Id', how='left')
print(f"User features merged: {ml_dataset.shape}")

# Merge dengan place features
place_columns_to_merge = ['Place_Id', 'Place_Name', 'Category', 'City', 'Price_Numeric',
                         'Rating', 'Time_Minutes', 'price_category', 'province',
                         'place_avg_rating', 'place_rating_std', 'place_total_ratings',
                         'place_is_popular', 'place_is_highly_rated', 'popularity_score']

available_place_cols = [col for col in place_columns_to_merge if col in place_features_complete.columns]
ml_dataset = ml_dataset.merge(place_features_complete[available_place_cols],
                             on='Place_Id', how='left')
print(f"Place features merged: {ml_dataset.shape}")

print()

"""## Rating deviation features"""

# Seberapa jauh rating user dari rata-rata mereka sendiri
ml_dataset['rating_vs_user_avg'] = (ml_dataset['Place_Ratings'] -
                                   ml_dataset['user_avg_rating']).fillna(0)

# Seberapa jauh rating dari rata-rata tempat tersebut
ml_dataset['rating_vs_place_avg'] = (ml_dataset['Place_Ratings'] -
                                    ml_dataset['place_avg_rating']).fillna(0)

# Rating surprise: kombinasi kedua deviasi
ml_dataset['rating_surprise'] = (np.abs(ml_dataset['rating_vs_user_avg']) +
                                np.abs(ml_dataset['rating_vs_place_avg']))

"""## User-place price compatibility"""

# Apakah user dan tempat "cocok" dari segi budget
ml_dataset['user_place_price_fit'] = 0

# User dengan rating tinggi biasanya suka tempat premium
high_rating_mask = ml_dataset['user_avg_rating'] > 3.5
expensive_mask = ml_dataset['Price_Numeric'] > 100000
ml_dataset.loc[high_rating_mask & expensive_mask, 'user_place_price_fit'] = 1

# User dengan rating rendah biasanya suka tempat murah/gratis
low_rating_mask = ml_dataset['user_avg_rating'] <= 3.5
cheap_mask = ml_dataset['Price_Numeric'] <= 50000
ml_dataset.loc[low_rating_mask & cheap_mask, 'user_place_price_fit'] = 1

"""## Age-category compatibility"""

ml_dataset['age_category_match'] = 0

# Define preferences berdasarkan analisis umum
teen_categories = ['Taman Hiburan', 'Pantai', 'Pusat Perbelanjaan']
young_adult_categories = ['Pantai', 'Gunung', 'Taman Hiburan', 'Danau']
adult_categories = ['Cagar Alam', 'Budaya', 'Gunung', 'Museum']
mature_categories = ['Budaya', 'Cagar Alam', 'Tempat Ibadah', 'Museum']

# Apply matching logic
teen_mask = ml_dataset['age_group'] == 'Teen'
young_mask = ml_dataset['age_group'] == 'Young_Adult'
adult_mask = ml_dataset['age_group'] == 'Adult'
mature_mask = ml_dataset['age_group'].isin(['Middle_Age', 'Senior', 'Elder'])

ml_dataset.loc[teen_mask & ml_dataset['Category'].isin(teen_categories), 'age_category_match'] = 1
ml_dataset.loc[young_mask & ml_dataset['Category'].isin(young_adult_categories), 'age_category_match'] = 1
ml_dataset.loc[adult_mask & ml_dataset['Category'].isin(adult_categories), 'age_category_match'] = 1
ml_dataset.loc[mature_mask & ml_dataset['Category'].isin(mature_categories), 'age_category_match'] = 1

"""## Time-based features"""

if 'Time_Minutes' in ml_dataset.columns:
    time_median = ml_dataset['Time_Minutes'].median()
    ml_dataset['is_long_visit'] = (ml_dataset['Time_Minutes'] > time_median).astype(int)

    # Time-value ratio: berapa lama waktu per rupiah yang dihabiskan
    ml_dataset['time_value_ratio'] = ml_dataset['Time_Minutes'] / (ml_dataset['Price_Numeric'] + 1)

"""## Location preference"""

# Apakah user suka wisata lokal atau luar daerah
if 'Location' in ml_dataset.columns and 'City' in ml_dataset.columns:
    ml_dataset['is_local'] = (ml_dataset['Location'] == ml_dataset['City']).astype(int)
    print("Location preference features")

print(f"Final interaction dataset: {ml_dataset.shape}")
print()

"""# MISSING VALUES HANDLING"""

# Check missing values
missing_summary = ml_dataset.isnull().sum()
missing_cols = missing_summary[missing_summary > 0]

if len(missing_cols) > 0:
    print("Missing values ditemukan:")
    for col, count in missing_cols.items():
        percentage = (count / len(ml_dataset)) * 100
        print(f" {col}: {count:,} ({percentage:.1f}%)")

    print("\nMengisi missing values...")

    # Handle berdasarkan tipe data
    for col in missing_cols.index:
        if ml_dataset[col].dtype in ['int64', 'float64']:
            # Numerical: gunakan median
            fill_value = ml_dataset[col].median()
            ml_dataset[col] = ml_dataset[col].fillna(fill_value)
            print(f" {col}: diisi dengan median ({fill_value})")
        else:
            # Categorical: gunakan mode atau 'Unknown'
            if not ml_dataset[col].mode().empty:
                fill_value = ml_dataset[col].mode()[0]
                ml_dataset[col] = ml_dataset[col].fillna(fill_value)
                print(f" {col}: diisi dengan mode ('{fill_value}')")
            else:
                ml_dataset[col] = ml_dataset[col].fillna('Unknown')
                print(f" {col}: diisi dengan 'Unknown'")
else:
    print("Tidak ada missing values!")

# Final check
final_missing = ml_dataset.isnull().sum().sum()
print(f"\nFinal missing values: {final_missing}")
print()

"""# CATEGORICAL ENCODING"""

# Identifikasi kolom categorical
categorical_columns = ['Category', 'City', 'price_category', 'age_group', 'province', 'Location']
categorical_in_data = [col for col in categorical_columns if col in ml_dataset.columns]

print(f"Kolom categorical: {categorical_in_data}")

# Initialize label encoders untuk production use
label_encoders = {}

# Encode setiap kolom categorical
for col in categorical_in_data:
    print(f"Encoding {col}:")

    # Buat label encoder
    le = LabelEncoder()

    # Fit dan transform
    ml_dataset[f'{col}_encoded'] = le.fit_transform(ml_dataset[col].astype(str))

    # Simpan encoder untuk production
    label_encoders[col] = le

    # Info encoding
    unique_count = len(le.classes_)
    print(f"{unique_count} kategori unik berhasil di encode")

    # Tampilkan mapping untuk kolom dengan kategori sedikit
    if unique_count <= 10:
        mapping = dict(zip(le.classes_, le.transform(le.classes_)))
        print(f"Mapping: {mapping}\n")

print(f"\n{len(categorical_in_data)} kolom categorical berhasil di encode")
print()

"""# FEATURE MATRIX PREPARATION"""

# Definisikan grup features
numerical_features = [
    'Age', 'Price_Numeric', 'Rating', 'Time_Minutes',
    'user_avg_rating', 'user_rating_std', 'user_total_ratings',
    'place_avg_rating', 'place_rating_std', 'place_total_ratings',
    'rating_vs_user_avg', 'rating_vs_place_avg', 'rating_surprise',
    'popularity_score', 'time_value_ratio'
]

# Features yang sudah di-encode
encoded_features = [f'{col}_encoded' for col in categorical_in_data]

# Binary features
binary_features = [
    'user_is_generous', 'user_is_active', 'user_is_consistent', 'user_is_explorer',
    'place_is_popular', 'place_is_highly_rated', 'place_is_consistent', 'place_is_niche',
    'user_place_price_fit', 'age_category_match', 'is_long_visit', 'is_local'
]

# Filter features yang benar-benar ada di dataset
available_numerical = [col for col in numerical_features if col in ml_dataset.columns]
available_binary = [col for col in binary_features if col in ml_dataset.columns]

print(f"Feature summary:")
print(f"   Numerical features: {len(available_numerical)}")
print(f"   Encoded features: {len(encoded_features)}")
print(f"   Binary features: {len(available_binary)}")

# Buat daftar lengkap features
all_feature_columns = available_numerical + encoded_features + available_binary

print(f"Total features: {len(all_feature_columns)}")

# Buat feature matrix lengkap dengan ID untuk tracking
X_full = ml_dataset[['User_Id', 'Place_Id'] + all_feature_columns].copy()
y = ml_dataset['Place_Ratings'].copy()

# Feature matrix tanpa ID untuk ML
X_features = X_full.drop(['User_Id', 'Place_Id'], axis=1)

print(f"Feature matrix siap:")
print(f"      Samples: {X_features.shape[0]:,}")
print(f"      Features: {X_features.shape[1]}")
print(f"      Target shape: {y.shape[0]:,}")

# Tampilkan distribusi target variable
target_dist = y.value_counts().sort_index()
print(f"Target distribution: {target_dist}")
print()

"""# TRAIN-TEST SPLIT (USER-AWARE)"""

print("Melakukan user-aware split untuk recommendation system")

# Untuk recommendation system, kita perlu memastikan:
# 1. User yang sama bisa ada di train dan test (untuk cold start problem)
# 2. Tapi tidak ada data leakage antar split

# Grup berdasarkan user
user_groups = X_full.groupby('User_Id')
train_indices = []
test_indices = []

users_with_enough_data = 0
users_train_only = 0
total_users = len(user_groups)

print(f"Memproses {total_users:,} users")

# Set random seed untuk reproducibility
np.random.seed(42)

# Process setiap user
for user_id, user_data in user_groups:
    user_indices = user_data.index.tolist()
    n_interactions = len(user_indices)

    if n_interactions >= 5:  # User dengan interaksi cukup untuk split
        users_with_enough_data += 1

        # Shuffle interactions user ini
        user_seed = 42 + int(user_id) % 1000  # Biar reproducible per user
        np.random.seed(user_seed)
        shuffled_indices = np.random.permutation(user_indices)

        # 20% untuk test, minimal 1 interaction
        n_test = max(1, int(n_interactions * 0.2))
        test_indices.extend(shuffled_indices[:n_test])
        train_indices.extend(shuffled_indices[n_test:])

    else:
        # User dengan sedikit interaksi masuk training semua
        users_train_only += 1
        train_indices.extend(user_indices)

# Buat train-test datasets
X_train_full = X_full.loc[train_indices].copy()
X_test_full = X_full.loc[test_indices].copy()
y_train = y.loc[train_indices].copy()
y_test = y.loc[test_indices].copy()

# Feature matrices tanpa ID columns
X_train = X_train_full.drop(['User_Id', 'Place_Id'], axis=1)
X_test = X_test_full.drop(['User_Id', 'Place_Id'], axis=1)

# Split statistics
train_pct = len(X_train) / len(ml_dataset) * 100
test_pct = len(X_test) / len(ml_dataset) * 100

print(f"Split results:")
print(f"      Train set: {len(X_train):,} interactions ({train_pct:.1f}%)")
print(f"      Test set: {len(X_test):,} interactions ({test_pct:.1f}%)")
print(f"      Train users: {X_train_full['User_Id'].nunique():,}")
print(f"      Test users: {X_test_full['User_Id'].nunique():,}")
print(f"      Users with 5+ interactions: {users_with_enough_data:,}")
print(f"      Users in train only: {users_train_only:,}")

# Check distribusi target di kedua set
train_target_dist = y_train.value_counts().sort_index()
test_target_dist = y_test.value_counts().sort_index()
print(f"      Train target dist: {dict(train_target_dist)}")
print(f"      Test target dist: {dict(test_target_dist)}")
print()

"""# FEATURE SCALING"""

# Hanya scale numerical features
numerical_in_data = [col for col in available_numerical if col in X_train.columns]

if len(numerical_in_data) > 0:
    print(f"Scaling {len(numerical_in_data)} numerical features:")
    for feature in numerical_in_data:
        print(f"  - {feature}")

    # Initialize StandardScaler
    scaler = StandardScaler()

    # Buat scaled versions
    X_train_scaled = X_train.copy()
    X_test_scaled = X_test.copy()

    # Fit pada training data, transform keduanya
    X_train_scaled[numerical_in_data] = scaler.fit_transform(X_train[numerical_in_data])
    X_test_scaled[numerical_in_data] = scaler.transform(X_test[numerical_in_data])

    # Tampilkan scaling statistics
    scaling_stats = pd.DataFrame({
        'Feature': numerical_in_data,
        'Original_Mean': X_train[numerical_in_data].mean().round(4),
        'Scaled_Mean': X_train_scaled[numerical_in_data].mean().round(4),
        'Original_Std': X_train[numerical_in_data].std().round(4),
        'Scaled_Std': X_train_scaled[numerical_in_data].std().round(4)
    })

    print("\nScaling statistics:")
    display(scaling_stats)

else:
    print("No numerical features to scale")
    X_train_scaled = X_train.copy()
    X_test_scaled = X_test.copy()
    scaler = None

print()

"""# RECOMMENDATION MATRICES CREATION"""

# Training interaction matrix (explicit ratings)
train_data_for_matrix = X_train_full.copy()
train_data_for_matrix['rating'] = y_train

user_item_matrix_train = train_data_for_matrix.pivot_table(
    index='User_Id',
    columns='Place_Id',
    values='rating',
    fill_value=0
)

# Binary matrix untuk implicit feedback algorithms
user_item_binary_train = (user_item_matrix_train > 0).astype(int)

# Test matrix untuk evaluation
test_data_for_matrix = X_test_full.copy()
test_data_for_matrix['rating'] = y_test

user_item_matrix_test = test_data_for_matrix.pivot_table(
    index='User_Id',
    columns='Place_Id',
    values='rating',
    fill_value=0
)

# Matrix statistics untuk monitoring
matrix_users = user_item_matrix_train.shape[0]
matrix_items = user_item_matrix_train.shape[1]
total_cells = matrix_users * matrix_items
filled_cells = (user_item_matrix_train > 0).sum().sum()
sparsity = (1 - filled_cells / total_cells) * 100

print(f"Matrix statistics:")
print(f"      Training matrix: {matrix_users:,} users × {matrix_items:,} places")
print(f"      Test matrix: {user_item_matrix_test.shape}")
print(f"      Total possible interactions: {total_cells:,}")
print(f"      Actual interactions: {filled_cells:,}")
print(f"      Matrix sparsity: {sparsity:.2f}%")
print(f"      Avg interactions per user: {filled_cells / matrix_users:.1f}")
print(f"      Avg interactions per place: {filled_cells / matrix_items:.1f}")
print()

"""# DATA QUALITY VALIDATION"""

# Check data leakage
train_test_index_overlap = len(set(X_train.index) & set(X_test.index))

# Feature consistency
train_features = set(X_train_scaled.columns)
test_features = set(X_test_scaled.columns)
feature_mismatch = train_features.symmetric_difference(test_features)

# Missing values check
train_missing = X_train_scaled.isnull().sum().sum()
test_missing = X_test_scaled.isnull().sum().sum()

# Data types consistency
print(f"Data types consistency:")
dtype_issues = 0
for col in X_train_scaled.columns:
    if X_train_scaled[col].dtype != X_test_scaled[col].dtype:
        print(f"{col}: Train={X_train_scaled[col].dtype}, Test={X_test_scaled[col].dtype}")
        dtype_issues += 1

if dtype_issues == 0:
    print(f"All {len(X_train_scaled.columns)} features have consistent data types")

# Check data leakage
train_test_index_overlap = len(set(X_train.index) & set(X_test.index))
print(f"Data leakage check: {train_test_index_overlap} overlapping indices (should be 0)")

# Feature consistency
train_features = set(X_train_scaled.columns)
test_features = set(X_test_scaled.columns)
feature_mismatch = train_features.symmetric_difference(test_features)
print(f"Feature consistency: {len(feature_mismatch)} mismatched features (should be 0)")

# Missing values check
train_missing = X_train_scaled.isnull().sum().sum()
test_missing = X_test_scaled.isnull().sum().sum()
print(f"Missing values: Train={train_missing}, Test={test_missing} (should be 0)")

# Data types consistency
print(f"Data types consistency:")
dtype_issues = 0
for col in X_train_scaled.columns:
    if X_train_scaled[col].dtype != X_test_scaled[col].dtype:
        print(f"{col}: Train={X_train_scaled[col].dtype}, Test={X_test_scaled[col].dtype}")
        dtype_issues += 1

if dtype_issues == 0:
    print(f"All {len(X_train_scaled.columns)} features have consistent data types")

# Value ranges check untuk scaled features
if scaler is not None:
    print(f"Scaled features validation:")
    for col in numerical_in_data:
        train_mean = X_train_scaled[col].mean()
        train_std = X_train_scaled[col].std()
        test_mean = X_test_scaled[col].mean()
        test_std = X_test_scaled[col].std()

        # Check jika scaling berhasil (mean = 0, std = 1 untuk train)
        if abs(train_mean) > 0.01:  # Threshold untuk floating point precision
            print(f"{col}: Train mean = {train_mean:.4f} (should be ~0)")

        if abs(train_std - 1.0) > 0.01:
            print(f"{col}: Train std = {train_std:.4f} (should be ~1)")

print(f"Data quality validation completed")
print()

"""# POSTGRESQL DATABASE CONNECTION & EXPORT"""

!pip install mlflow boto3 -q

import pandas as pd
import numpy as np
import json
import tempfile
import os
from datetime import datetime
import psycopg2
from sqlalchemy import create_engine, text
import mlflow
import joblib
import warnings
warnings.filterwarnings('ignore')

# KONFIGURASI - sesuaikan dengan setup Anda
MINIO_CONFIG = {
    'endpoint': 'http://103.150.98.118:9000',
    'access_key': 'minioadmin',
    'secret_key': 'minioadminpass13',
    'bucket_name': 'tourism-data'
}

POSTGRES_CONFIG = {
    'host': '103.150.98.118',
    'port': 5432,
    'database': 'mlflowdb',
    'user': 'mlflow',
    'password': 'group13',
    'schema': 'tourism_data'
}

def setup_minio_environment():
    """Setup environment untuk MinIO"""
    os.environ['AWS_ACCESS_KEY_ID'] = MINIO_CONFIG['access_key']
    os.environ['AWS_SECRET_ACCESS_KEY'] = MINIO_CONFIG['secret_key']
    os.environ['MLFLOW_S3_ENDPOINT_URL'] = MINIO_CONFIG['endpoint']
    os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'
    os.environ['AWS_S3_ENDPOINT_URL'] = MINIO_CONFIG['endpoint']
    print("MinIO environment configured")

def create_postgresql_connection():
    """Create PostgreSQL connection"""
    try:
        # Connection string untuk SQLAlchemy
        connection_string = f"postgresql://{POSTGRES_CONFIG['user']}:{POSTGRES_CONFIG['password']}@{POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}/{POSTGRES_CONFIG['database']}"

        engine = create_engine(connection_string)

        # Test connection
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))

        print("✅ PostgreSQL connection successful")
        return engine

    except Exception as e:
        print(f"❌ PostgreSQL connection failed: {e}")
        return None

def save_to_postgresql(df, table_name, engine, schema='tourism_data'):
    """Save dataframe to PostgreSQL"""
    try:
        # Create schema if not exists
        with engine.connect() as conn:
            conn.execute(text(f"CREATE SCHEMA IF NOT EXISTS {schema}"))
            conn.commit()

        # Save dataframe
        df.to_sql(
            name=table_name,
            con=engine,
            schema=schema,
            if_exists='replace',  # Replace table if exists
            index=False,
            chunksize=1000
        )

        print(f"✅ {table_name}: {len(df):,} rows saved to PostgreSQL")
        return True

    except Exception as e:
        print(f"❌ Failed to save {table_name} to PostgreSQL: {e}")
        return False

def save_to_minio_via_mlflow(df, filename):
    """Save dataframe to MinIO via MLflow artifacts"""
    try:
        # Setup MLflow untuk MinIO
        mlflow.set_tracking_uri('http://103.150.98.118:5000')
        mlflow.set_experiment('Tourism_Data_Storage')

        with tempfile.TemporaryDirectory() as temp_dir:
            # Save as CSV
            csv_path = os.path.join(temp_dir, f"{filename}.csv")
            df.to_csv(csv_path, index=False)

            # Save as Parquet (more efficient)
            parquet_path = os.path.join(temp_dir, f"{filename}.parquet")
            df.to_parquet(parquet_path, index=False)

            # Log to MLflow (which stores in MinIO)
            with mlflow.start_run(run_name=f"data_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
                mlflow.log_artifact(csv_path, "csv_data")
                mlflow.log_artifact(parquet_path, "parquet_data")

                # Log metadata
                metadata = {
                    'filename': filename,
                    'rows': len(df),
                    'columns': list(df.columns),
                    'dtypes': df.dtypes.to_dict(),
                    'timestamp': datetime.now().isoformat()
                }

                metadata_path = os.path.join(temp_dir, f"{filename}_metadata.json")
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2, default=str)

                mlflow.log_artifact(metadata_path, "metadata")
                mlflow.log_param("dataset", filename)
                mlflow.log_metric("row_count", len(df))
                mlflow.log_metric("column_count", len(df.columns))

        print(f"✅ {filename}: {len(df):,} rows saved to MinIO via MLflow")
        return True

    except Exception as e:
        print(f"❌ Failed to save {filename} to MinIO: {e}")
        return False

def main_data_export():
    """Main function untuk export semua data clean"""
    print("🚀 TOURISM DATA EXPORT TO POSTGRESQL & MINIO")
    print("="*60)

    # Setup environments
    setup_minio_environment()
    engine = create_postgresql_connection()

    if engine is None:
        print("❌ Cannot proceed without database connection")
        return False

    # Daftar dataframes yang akan disimpan dari code Anda
    datasets_to_save = []

    # 1. Dataset Tourism yang sudah dibersihkan
    if 'place_features_complete' in globals():
        datasets_to_save.append({
            'df': place_features_complete,
            'name': 'tourism_places_clean',
            'description': 'Tourism places with complete features and statistics'
        })
    elif 'df_tourism_clean' in globals():
        datasets_to_save.append({
            'df': df_tourism_clean,
            'name': 'tourism_places_clean',
            'description': 'Clean tourism places data'
        })

    # 2. Dataset Users yang sudah dienhance
    if 'user_features_complete' in globals():
        datasets_to_save.append({
            'df': user_features_complete,
            'name': 'users_enhanced',
            'description': 'Users with enhanced features and behavior analysis'
        })
    elif 'df_users_enhanced' in globals():
        datasets_to_save.append({
            'df': df_users_enhanced,
            'name': 'users_enhanced',
            'description': 'Enhanced users data'
        })

    # 3. Dataset Ratings
    if 'df_ratings' in globals():
        datasets_to_save.append({
            'df': df_ratings,
            'name': 'ratings_data',
            'description': 'User ratings for tourism places'
        })

    # 4. Dataset ML yang sudah siap
    if 'ml_dataset' in globals():
        datasets_to_save.append({
            'df': ml_dataset,
            'name': 'ml_ready_dataset',
            'description': 'Complete ML dataset with all features'
        })

    # 5. Train-Test datasets
    if 'X_train_full' in globals() and 'y_train' in globals():
        # Gabungkan X_train_full dengan y_train
        train_dataset = X_train_full.copy()
        train_dataset['target_rating'] = y_train
        datasets_to_save.append({
            'df': train_dataset,
            'name': 'train_dataset',
            'description': 'Training dataset with features and target'
        })

    if 'X_test_full' in globals() and 'y_test' in globals():
        # Gabungkan X_test_full dengan y_test
        test_dataset = X_test_full.copy()
        test_dataset['target_rating'] = y_test
        datasets_to_save.append({
            'df': test_dataset,
            'name': 'test_dataset',
            'description': 'Test dataset with features and target'
        })

    # 6. User-Item matrices
    if 'user_item_matrix_train' in globals():
        # Convert matrix to long format untuk storage yang lebih efisien
        matrix_long = user_item_matrix_train.reset_index().melt(
            id_vars=['User_Id'],
            var_name='Place_Id',
            value_name='rating'
        )
        # Hanya simpan yang ada ratingnya
        matrix_long = matrix_long[matrix_long['rating'] > 0]
        datasets_to_save.append({
            'df': matrix_long,
            'name': 'user_item_matrix',
            'description': 'User-Item interaction matrix in long format'
        })

    # 7. Feature statistics dan metadata
    if 'X_train_scaled' in globals():
        # Buat summary statistics dari features
        feature_stats = pd.DataFrame({
            'feature_name': X_train_scaled.columns,
            'mean': X_train_scaled.mean(),
            'std': X_train_scaled.std(),
            'min': X_train_scaled.min(),
            'max': X_train_scaled.max(),
            'dtype': X_train_scaled.dtypes.astype(str)
        }).reset_index(drop=True)

        datasets_to_save.append({
            'df': feature_stats,
            'name': 'feature_statistics',
            'description': 'Statistical summary of ML features'
        })

    # Check apakah ada data untuk disimpan
    if not datasets_to_save:
        print("❌ No clean datasets found to save!")
        print("Make sure you've run the data cleaning and feature engineering sections.")
        return False

    print(f"📊 Found {len(datasets_to_save)} datasets to save:")
    for dataset in datasets_to_save:
        print(f"   • {dataset['name']}: {len(dataset['df']):,} rows, {len(dataset['df'].columns)} columns")

    # Save to PostgreSQL
    print(f"\n💾 SAVING TO POSTGRESQL")
    print("-" * 30)
    postgres_success = 0
    for dataset in datasets_to_save:
        if save_to_postgresql(dataset['df'], dataset['name'], engine):
            postgres_success += 1

    # Save to MinIO
    print(f"\n☁️ SAVING TO MINIO VIA MLFLOW")
    print("-" * 35)
    minio_success = 0
    for dataset in datasets_to_save:
        if save_to_minio_via_mlflow(dataset['df'], dataset['name']):
            minio_success += 1

    # Save preprocessing objects
    print(f"\n🔧 SAVING PREPROCESSING OBJECTS")
    print("-" * 38)

    preprocessing_objects = {}

    if 'scaler' in globals() and scaler is not None:
        preprocessing_objects['scaler'] = scaler

    if 'label_encoders' in globals():
        preprocessing_objects['label_encoders'] = label_encoders

    if 'all_feature_columns' in globals():
        preprocessing_objects['feature_columns'] = all_feature_columns

    if preprocessing_objects:
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                with mlflow.start_run(run_name=f"preprocessing_objects_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
                    for obj_name, obj in preprocessing_objects.items():
                        obj_path = os.path.join(temp_dir, f"{obj_name}.pkl")
                        joblib.dump(obj, obj_path)
                        mlflow.log_artifact(obj_path, "preprocessing")
                        print(f"✅ {obj_name} saved to MinIO")
        except Exception as e:
            print(f"❌ Failed to save preprocessing objects: {e}")

    # Summary
    print(f"\n🎉 EXPORT SUMMARY")
    print("=" * 20)
    print(f"📊 Total datasets: {len(datasets_to_save)}")
    print(f"💾 PostgreSQL success: {postgres_success}/{len(datasets_to_save)}")
    print(f"☁️ MinIO success: {minio_success}/{len(datasets_to_save)}")

    if postgres_success == len(datasets_to_save) and minio_success == len(datasets_to_save):
        print("🏆 ALL DATA EXPORTED SUCCESSFULLY!")

        # Provide access information
        print(f"\n📋 ACCESS INFORMATION:")
        print(f"🗄️ PostgreSQL:")
        print(f"   Host: {POSTGRES_CONFIG['host']}")
        print(f"   Database: {POSTGRES_CONFIG['database']}")
        print(f"   Schema: tourism_data")
        print(f"☁️ MinIO/MLflow:")
        print(f"   MLflow UI: http://103.150.98.118:5000")
        print(f"   MinIO Console: http://103.150.98.118:9000")

        return True
    else:
        print("⚠️ Some exports failed. Check error messages above.")
        return False

# Execute the export
if __name__ == "__main__":
    success = main_data_export()

    if success:
        print(f"\n✨ MISSION ACCOMPLISHED!")
        print("Your clean tourism datasets are now safely stored!")
    else:
        print(f"\n💥 EXPORT INCOMPLETE")
        print("Check the error messages above for troubleshooting.")

"""# MACHINE LEARNING"""

!pip install mlflow -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import time
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor


import lightgbm as lgb
import xgboost as xgb
import mlflow
import mlflow.sklearn
import mlflow.xgboost
import mlflow.lightgbm

"""## LIGHTGBM MODEL"""

try:
    # Validate input data
    if X_train_scaled is None or y_train is None:
        raise ValueError("Training data not properly initialized")
    if X_test_scaled is None or y_test is None:
        raise ValueError("Test data not properly initialized")

    print(f"Training samples: {len(X_train_scaled):,}")
    print(f"Test samples: {len(X_test_scaled):,}")
    print(f"Features: {X_train_scaled.shape[1]}")

    # Create and train LightGBM model
    lgb_model = lgb.LGBMRegressor(
        objective='regression',
        num_leaves=31,
        learning_rate=0.05,
        n_estimators=100,
        random_state=42,
        verbose=-1
    )

    print("Training LightGBM...")
    start_time = time.time()
    lgb_model.fit(X_train_scaled, y_train)
    lgb_training_time = time.time() - start_time

    # Make predictions
    lgb_pred_train = lgb_model.predict(X_train_scaled)
    lgb_pred_test = lgb_model.predict(X_test_scaled)

    # Calculate metrics
    lgb_train_r2 = r2_score(y_train, lgb_pred_train)
    lgb_test_r2 = r2_score(y_test, lgb_pred_test)
    lgb_train_rmse = np.sqrt(mean_squared_error(y_train, lgb_pred_train))
    lgb_test_rmse = np.sqrt(mean_squared_error(y_test, lgb_pred_test))
    lgb_train_mae = mean_absolute_error(y_train, lgb_pred_train)
    lgb_test_mae = mean_absolute_error(y_test, lgb_pred_test)

    # Print results
    print(f"\nLightGBM Training completed in {lgb_training_time:.2f} seconds")
    print("\nLightGBM Model Performance:")
    print("{:<15} {:<10} {:<10}".format("Metric", "Train", "Test"))
    print("-" * 35)
    print("{:<15} {:<10.4f} {:<10.4f}".format("R2", lgb_train_r2, lgb_test_r2))
    print("{:<15} {:<10.4f} {:<10.4f}".format("RMSE", lgb_train_rmse, lgb_test_rmse))
    print("{:<15} {:<10.4f} {:<10.4f}".format("MAE", lgb_train_mae, lgb_test_mae))

    lgb_success = True
    print("LightGBM training successful!")

except Exception as e:
    print(f"Error in LightGBM training: {str(e)}")
    lgb_success = False

"""## RANDOM FOREST MODEL"""

try:
    # Create Random Forest model
    rf_model = RandomForestRegressor(
        n_estimators=100,
        max_depth=20,
        random_state=42,
        n_jobs=-1
    )

    print("Training Random Forest...")
    start_time = time.time()
    rf_model.fit(X_train_scaled, y_train)
    rf_training_time = time.time() - start_time

    # Make predictions
    rf_pred_train = rf_model.predict(X_train_scaled)
    rf_pred_test = rf_model.predict(X_test_scaled)

    # Calculate metrics
    rf_train_r2 = r2_score(y_train, rf_pred_train)
    rf_test_r2 = r2_score(y_test, rf_pred_test)
    rf_train_rmse = np.sqrt(mean_squared_error(y_train, rf_pred_train))
    rf_test_rmse = np.sqrt(mean_squared_error(y_test, rf_pred_test))
    rf_train_mae = mean_absolute_error(y_train, rf_pred_train)
    rf_test_mae = mean_absolute_error(y_test, rf_pred_test)

    # Print results
    print(f"\nRandom Forest Training completed in {rf_training_time:.2f} seconds")
    print("\nRandom Forest Model Performance:")
    print("{:<15} {:<10} {:<10}".format("Metric", "Train", "Test"))
    print("-" * 35)
    print("{:<15} {:<10.4f} {:<10.4f}".format("R2", rf_train_r2, rf_test_r2))
    print("{:<15} {:<10.4f} {:<10.4f}".format("RMSE", rf_train_rmse, rf_test_rmse))
    print("{:<15} {:<10.4f} {:<10.4f}".format("MAE", rf_train_mae, rf_test_mae))

    rf_success = True
    print("Random Forest training successful!")

except Exception as e:
    print(f"Error in Random Forest training: {str(e)}")
    rf_success = False

"""## XGBOOST MODEL"""

try:
    # Create XGBoost model
    xgb_model = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        n_jobs=-1
    )

    print("Training XGBoost...")
    start_time = time.time()
    xgb_model.fit(X_train_scaled, y_train)
    xgb_training_time = time.time() - start_time

    # Make predictions
    xgb_pred_train = xgb_model.predict(X_train_scaled)
    xgb_pred_test = xgb_model.predict(X_test_scaled)

    # Calculate metrics
    xgb_train_r2 = r2_score(y_train, xgb_pred_train)
    xgb_test_r2 = r2_score(y_test, xgb_pred_test)
    xgb_train_rmse = np.sqrt(mean_squared_error(y_train, xgb_pred_train))
    xgb_test_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred_test))
    xgb_train_mae = mean_absolute_error(y_train, xgb_pred_train)
    xgb_test_mae = mean_absolute_error(y_test, xgb_pred_test)

    # Print results
    print(f"\nXGBoost Training completed in {xgb_training_time:.2f} seconds")
    print("\nXGBoost Model Performance:")
    print("{:<15} {:<10} {:<10}".format("Metric", "Train", "Test"))
    print("-" * 35)
    print("{:<15} {:<10.4f} {:<10.4f}".format("R2", xgb_train_r2, xgb_test_r2))
    print("{:<15} {:<10.4f} {:<10.4f}".format("RMSE", xgb_train_rmse, xgb_test_rmse))
    print("{:<15} {:<10.4f} {:<10.4f}".format("MAE", xgb_train_mae, xgb_test_mae))

    xgb_success = True
    print("XGBoost training successful!")

except Exception as e:
    print(f"Error in XGBoost training: {str(e)}")
    xgb_success = False

"""# COMPARE ALL MODELS"""

# Collect results from all successful models
results_data = []

if lgb_success:
    results_data.append({
        'Model': 'LightGBM',
        'Test_R2': lgb_test_r2,
        'Test_RMSE': lgb_test_rmse,
        'Test_MAE': lgb_test_mae,
        'Training_Time': lgb_training_time
    })

if rf_success:
    results_data.append({
        'Model': 'Random Forest',
        'Test_R2': rf_test_r2,
        'Test_RMSE': rf_test_rmse,
        'Test_MAE': rf_test_mae,
        'Training_Time': rf_training_time
    })

if xgb_success:
    results_data.append({
        'Model': 'XGBoost',
        'Test_R2': xgb_test_r2,
        'Test_RMSE': xgb_test_rmse,
        'Test_MAE': xgb_test_mae,
        'Training_Time': xgb_training_time
    })

if not results_data:
    raise RuntimeError("No models trained successfully!")

# Create comparison dataframe
results_df = pd.DataFrame(results_data)
results_df = results_df.sort_values('Test_R2', ascending=False)

print("Model Performance Comparison:")
print(results_df.round(4).to_string(index=False))

# Determine best model
best_model_row = results_df.iloc[0]
best_model_name = best_model_row['Model']
best_r2_score = best_model_row['Test_R2']

# Get the actual model object
model_mapping = {}
if lgb_success:
    model_mapping['LightGBM'] = lgb_model
if rf_success:
    model_mapping['Random Forest'] = rf_model
if xgb_success:
    model_mapping['XGBoost'] = xgb_model

best_model = model_mapping[best_model_name]

print(f"\nBEST MODEL SELECTED: {best_model_name}")
print(f"Best R2 Score: {best_r2_score:.4f}")
print(f"Best RMSE: {best_model_row['Test_RMSE']:.4f}")
print(f"Best MAE: {best_model_row['Test_MAE']:.4f}")

"""# SETUP CONNECTION"""

!pip install mlflow==2.11.1 -q
!pip install boto3 psycopg2-binary -q

import os
import json
import mlflow
import mlflow.sklearn
import mlflow.lightgbm
import mlflow.xgboost
from datetime import datetime
import tempfile
import warnings
import joblib
import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

warnings.filterwarnings('ignore')

MLFLOW_CONFIG = {
    'tracking_uri': 'http://103.150.98.118:5000',
    'minio_endpoint': 'http://103.150.98.118:9000',
    'minio_access_key': 'minioadmin',
    'minio_secret_key': 'minioadminpass13',
    'experiment_name': 'Indonesia_Tourism_Colab',
    'model_name': 'indonesia_tourism_best_model'
}

def setup_environment():
    try:
        # Set environment variables for MinIO
        os.environ['AWS_ACCESS_KEY_ID'] = MLFLOW_CONFIG['minio_access_key']
        os.environ['AWS_SECRET_ACCESS_KEY'] = MLFLOW_CONFIG['minio_secret_key']
        os.environ['MLFLOW_S3_ENDPOINT_URL'] = MLFLOW_CONFIG['minio_endpoint']
        os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'
        os.environ['AWS_S3_ENDPOINT_URL'] = MLFLOW_CONFIG['minio_endpoint']

        # Set tracking URI
        mlflow.set_tracking_uri(MLFLOW_CONFIG['tracking_uri'])

        # Test connection with basic client
        from mlflow.tracking import MlflowClient
        client = MlflowClient()
        experiments = client.search_experiments()
        print(f"   ✅ Connected! Found {len(experiments)} experiments")

        return True, client

    except Exception as e:
        print(f"   ❌ Connection failed: {e}")
        return False, None

def get_or_create_experiment(client, experiment_name):
    """Get or create experiment safely"""
    try:
        # Try to get experiment by name
        experiment = client.get_experiment_by_name(experiment_name)
        if experiment is not None:
            print(f"📁 Using existing experiment: {experiment_name}")
            return experiment.experiment_id
        else:
            # Create new experiment
            experiment_id = client.create_experiment(experiment_name)
            print(f"📁 Created new experiment: {experiment_name}")
            return experiment_id
    except Exception as e:
        print(f"⚠️ Experiment handling error: {e}")
        # Use default experiment as fallback
        return "0"

def log_model_fixed_method():
    print("-"*50)

    try:
        # Validate prerequisites
        required_vars = ['best_model', 'best_model_name', 'X_test_scaled', 'y_test']
        missing = [var for var in required_vars if var not in globals()]
        if missing:
            print(f"❌ Missing variables: {missing}")
            return False, None

        # Setup environment
        setup_success, client = setup_environment()
        if not setup_success:
            return False, None

        # Calculate metrics
        y_pred = best_model.predict(X_test_scaled)
        test_r2 = float(r2_score(y_test, y_pred))
        test_rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))
        test_mae = float(mean_absolute_error(y_test, y_pred))

        # Get or create experiment
        experiment_id = get_or_create_experiment(client, MLFLOW_CONFIG['experiment_name'])

        # Start run using client directly
        run = client.create_run(experiment_id)
        run_id = run.info.run_id
        print(f"🏃 Started Run: {run_id}")

        # Log parameters using client
        params = {
            "model_type": best_model_name,
            "train_samples": len(X_train_scaled) if 'X_train_scaled' in globals() else 0,
            "test_samples": len(X_test_scaled),
            "features_count": X_test_scaled.shape[1],
            "created_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "method": "fixed_compatible_logging"
        }

        for key, value in params.items():
            client.log_param(run_id, key, str(value))
        print("✅ Parameters logged")

        # Log metrics using client
        metrics = {
            "test_r2": test_r2,
            "test_rmse": test_rmse,
            "test_mae": test_mae
        }

        for key, value in metrics.items():
            client.log_metric(run_id, key, value)
        print("✅ Metrics logged")

        # Log artifacts manually (avoid problematic model logging APIs)
        with tempfile.TemporaryDirectory() as temp_dir:
            try:
                # Save model as joblib pickle
                model_file = os.path.join(temp_dir, "model.pkl")
                joblib.dump(best_model, model_file)
                client.log_artifact(run_id, model_file, "model")
                print("✅ Model saved as pickle")

                # Save scaler if exists
                if 'scaler' in globals() and scaler is not None:
                    scaler_file = os.path.join(temp_dir, "scaler.pkl")
                    joblib.dump(scaler, scaler_file)
                    client.log_artifact(run_id, scaler_file, "preprocessing")
                    print("✅ Scaler saved")

                # Save label encoders if exists
                if 'label_encoders' in globals():
                    encoders_file = os.path.join(temp_dir, "label_encoders.pkl")
                    joblib.dump(label_encoders, encoders_file)
                    client.log_artifact(run_id, encoders_file, "preprocessing")
                    print("✅ Label encoders saved")

                # Save feature names
                if 'X_train_scaled' in globals() and hasattr(X_train_scaled, 'columns'):
                    feature_names = list(X_train_scaled.columns)
                    feature_file = os.path.join(temp_dir, "feature_names.json")
                    with open(feature_file, 'w') as f:
                        json.dump(feature_names, f, indent=2)
                    client.log_artifact(run_id, feature_file, "preprocessing")
                    print("✅ Feature names saved")

                # Create comprehensive model info
                model_info = {
                    "model_type": best_model_name,
                    "model_class": str(type(best_model).__name__),
                    "performance": {
                        "test_r2_score": test_r2,
                        "test_rmse": test_rmse,
                        "test_mae": test_mae
                    },
                    "data_info": {
                        "train_samples": len(X_train_scaled) if 'X_train_scaled' in globals() else 0,
                        "test_samples": len(X_test_scaled),
                        "n_features": X_test_scaled.shape[1],
                        "target_range": {
                            "min": float(y_test.min()),
                            "max": float(y_test.max()),
                            "mean": float(y_test.mean())
                        }
                    },
                    "artifacts": {
                        "model_file": "model/model.pkl",
                        "scaler_file": "preprocessing/scaler.pkl" if 'scaler' in globals() and scaler else None,
                        "encoders_file": "preprocessing/label_encoders.pkl" if 'label_encoders' in globals() else None,
                        "features_file": "preprocessing/feature_names.json"
                    },
                    "usage_guide": {
                        "load_model": "model = joblib.load('model.pkl')",
                        "load_scaler": "scaler = joblib.load('scaler.pkl') if needed",
                        "load_encoders": "encoders = joblib.load('label_encoders.pkl') if needed",
                        "prediction": "predictions = model.predict(processed_features)"
                    },
                    "metadata": {
                        "created_timestamp": datetime.now().isoformat(),
                        "mlflow_version": mlflow.__version__,
                        "python_environment": "Google Colab"
                    }
                }

                # Save model info
                info_file = os.path.join(temp_dir, "model_info.json")
                with open(info_file, 'w') as f:
                    json.dump(model_info, f, indent=2)
                client.log_artifact(run_id, info_file, "info")
                print("✅ Model info saved")

                # Create a simple deployment script
                deployment_script = f'''#!/usr/bin/env python3
"""
Deployment script for {best_model_name} Tourism Model
Generated automatically by MLflow logging
"""

import joblib
import pandas as pd
import numpy as np

def load_model():
    """Load the trained model and preprocessing objects"""
    model = joblib.load('model.pkl')

    # Load preprocessing if available
    try:
        scaler = joblib.load('scaler.pkl')
    except:
        scaler = None

    try:
        encoders = joblib.load('label_encoders.pkl')
    except:
        encoders = None

    return model, scaler, encoders

def predict(model, scaler, encoders, features):
    """Make predictions with the loaded model"""
    # Apply preprocessing if needed
    if scaler is not None:
        # Apply scaling to numerical features
        features_scaled = scaler.transform(features)
    else:
        features_scaled = features

    # Make predictions
    predictions = model.predict(features_scaled)
    return predictions

# Example usage
if __name__ == "__main__":
    print("Loading Tourism Recommendation Model...")
    model, scaler, encoders = load_model()
    print(f"Model loaded: {{type(model).__name__}}")
    print("Ready for predictions!")
'''

                script_file = os.path.join(temp_dir, "deploy_model.py")
                with open(script_file, 'w') as f:
                    f.write(deployment_script)
                client.log_artifact(run_id, script_file, "deployment")
                print("✅ Deployment script created")

            except Exception as artifact_error:
                print(f"⚠️ Some artifacts failed: {artifact_error}")

        # Finalize run
        client.set_terminated(run_id, "FINISHED")

        # Build URLs
        run_url = f"{MLFLOW_CONFIG['tracking_uri']}/#/experiments/{experiment_id}/runs/{run_id}"
        experiment_url = f"{MLFLOW_CONFIG['tracking_uri']}/#/experiments/{experiment_id}"

        print(f"\n🎉 SUCCESS! Model logged successfully!")
        print(f"📊 Performance Summary:")
        print(f"   • R² Score: {test_r2:.4f}")
        print(f"   • RMSE: {test_rmse:.4f}")
        print(f"   • MAE: {test_mae:.4f}")
        print(f"📍 MLflow Links:")
        print(f"   • Run: {run_url}")
        print(f"   • Experiment: {experiment_url}")

        return True, {
            'run_id': run_id,
            'experiment_id': experiment_id,
            'run_url': run_url,
            'experiment_url': experiment_url,
            'metrics': metrics,
            'model_info': model_info
        }

    except Exception as e:
        print(f"❌ Fixed method failed: {str(e)}")
        import traceback
        print(f"🐛 Detailed error: {traceback.format_exc()}")
        return False, None

def simple_artifact_method():
    """Ultra-simple method that just saves to artifacts directory"""
    print("\nDirect Artifact Upload")
    print("-"*50)

    try:
        # Setup basic connection
        mlflow.set_tracking_uri(MLFLOW_CONFIG['tracking_uri'])

        # Calculate metrics
        y_pred = best_model.predict(X_test_scaled)
        test_r2 = float(r2_score(y_test, y_pred))
        test_rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))
        test_mae = float(mean_absolute_error(y_test, y_pred))

        # Use basic mlflow run context
        mlflow.set_experiment(MLFLOW_CONFIG['experiment_name'])

        with mlflow.start_run(run_name=f"Simple_{datetime.now().strftime('%H%M%S')}") as run:
            # Log basic metrics and params
            mlflow.log_param("model_type", best_model_name)
            mlflow.log_param("method", "ultra_simple")
            mlflow.log_metric("test_r2", test_r2)
            mlflow.log_metric("test_rmse", test_rmse)
            mlflow.log_metric("test_mae", test_mae)

            # Save artifacts to temp directory and upload
            with tempfile.TemporaryDirectory() as temp_dir:
                # Model
                model_path = os.path.join(temp_dir, "tourism_model.pkl")
                joblib.dump(best_model, model_path)
                mlflow.log_artifact(model_path)

                # Metadata
                metadata = {
                    "model": best_model_name,
                    "r2": test_r2,
                    "rmse": test_rmse,
                    "mae": test_mae,
                    "timestamp": datetime.now().isoformat()
                }

                metadata_path = os.path.join(temp_dir, "metadata.json")
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2)
                mlflow.log_artifact(metadata_path)

            run_id = run.info.run_id
            experiment_id = run.info.experiment_id

        print(f"✅ Simple method succeeded!")
        print(f"📊 R² Score: {test_r2:.4f}")
        print(f"🔗 Run ID: {run_id}")

        return True, {
            'run_id': run_id,
            'experiment_id': experiment_id,
            'metrics': {'r2': test_r2, 'rmse': test_rmse, 'mae': test_mae}
        }

    except Exception as e:
        print(f"❌ Simple method failed: {str(e)}")
        return False, None

def main_execution():
    """Main execution with multiple fallback methods"""
    print("🚀 MLFLOW TOURISM MODEL LOGGING")
    print("="*55)

    # Check prerequisites
    required_vars = ['best_model', 'best_model_name', 'X_test_scaled', 'y_test']
    missing = [var for var in required_vars if var not in globals()]
    if missing:
        print(f"❌ Missing required variables: {missing}")
        print("Please ensure the model training section completed successfully.")
        return False

    print(f"🎯 Model to log: {best_model_name}")
    print(f"📊 Test samples: {len(X_test_scaled):,}")
    print(f"🔢 Features: {X_test_scaled.shape[1]}")

    # Try Fixed Method (most robust)
    print("\n" + "="*50)
    success, result = log_model_fixed_method()

    if success:
        print("\n🏆 FIXED METHOD SUCCESSFUL!")
        print("✅ Your tourism model is now in MLflow!")
        print(f"🌐 View at: {result['run_url']}")
        return True

    # Try Simple Method as fallback
    print("\n" + "="*50)
    print("🔄 Trying ultra-simple fallback method...")
    success, result = simple_artifact_method()

    if success:
        print("\n🏆 SIMPLE METHOD SUCCESSFUL!")
        print("✅ Model saved with basic artifacts!")
        return True

    # If all methods fail
    print("\n💥 ALL METHODS FAILED")
    print("🆘 Possible solutions:")
    print("1. Check MLflow server version compatibility")
    print("2. Verify network connectivity")
    print("3. Check MLflow server logs")
    print("4. Try updating MLflow client version")

    return False

# Execute if run as main script
if __name__ == "__main__":
    success = main_execution()

    if success:
        print(f"\n🎊 MISSION ACCOMPLISHED!")
        print(f"Your {globals().get('best_model_name', 'model')} is safely stored in MLflow!")
        print(f"🌍 Access dashboard: {MLFLOW_CONFIG['tracking_uri']}")
    else:
        print(f"\n💀 MISSION FAILED")
        print("Check the error messages above for troubleshooting guidance.")

# Run the main execution
main_execution()